{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 03 - Model Diagnostics & Convergence Troubleshooting\n",
    "\n",
    "Bayesian inference via MCMC only produces valid results when the sampler **converges**.\n",
    "Non-convergent chains explore different regions of the posterior and give contradictory estimates.\n",
    "This notebook:\n",
    "\n",
    "1. Loads the fitted model from notebook 02\n",
    "2. Runs a full diagnostic suite (R-hat, ESS, divergences, max tree depth)\n",
    "3. Interprets each metric visually\n",
    "4. Diagnoses the root cause of the convergence failure\n",
    "5. Implements remediation (tighter priors + better sampling settings)\n",
    "6. Refits a v2 model and compares results\n",
    "\n",
    "## Convergence Thresholds (CLAUDE.md)\n",
    "\n",
    "| Metric | Threshold | Meaning if violated |\n",
    "|--------|-----------|--------------------|\n",
    "| **R-hat** | < 1.01 | Chains disagree — parameter not identified |\n",
    "| **ESS (bulk)** | > 400 | Too few independent samples — estimates unreliable |\n",
    "| **Divergences** | = 0 | Sampler hit a pathological region of the posterior |\n",
    "| **Max tree depth** | < 100% | NUTS can't explore efficiently — step size too large |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "from mmm_demo.config import OUTPUTS_DIR, ModelConfig\n",
    "from mmm_demo.data import load_mmm_weekly_data\n",
    "from mmm_demo.diagnostics import ESS_THRESHOLD, RHAT_THRESHOLD, check_convergence\n",
    "from mmm_demo.model import fit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the Saved Model\n",
    "\n",
    "> **Prerequisite:** Run notebook 02 first to generate `outputs/models/mmm_fit_*.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weekly data (needed for posterior predictive checks later)\n",
    "df = load_mmm_weekly_data()\n",
    "config = ModelConfig()\n",
    "feature_cols = [config.date_column, *config.channel_columns, *config.control_columns]\n",
    "X = df[feature_cols]\n",
    "y = df[config.target_column]\n",
    "\n",
    "# Find most recent saved model\n",
    "model_dir = OUTPUTS_DIR / \"models\"\n",
    "model_files = sorted(model_dir.glob(\"mmm_fit_*.nc\"))\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(\n",
    "        \"No saved model found in outputs/models/. Run notebook 02 first.\"\n",
    "    )\n",
    "\n",
    "model_path = model_files[-1]\n",
    "print(f\"Loading: {model_path.name}\")\n",
    "mmm = MMM.load(str(model_path))\n",
    "idata = mmm.idata\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "print(f\"Chains: {idata.posterior.dims['chain']}, Draws: {idata.posterior.dims['draw']}\")\n",
    "print(f\"Parameters: {list(idata.posterior.data_vars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = check_convergence(idata)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONVERGENCE DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall:     {'PASSED' if diag.passed else 'FAILED'}\")\n",
    "print()\n",
    "print(\n",
    "    f\"R-hat        max={diag.max_rhat:.4f}  threshold<{RHAT_THRESHOLD}  {'OK' if diag.rhat_ok else 'FAIL'}\"\n",
    ")\n",
    "print(\n",
    "    f\"ESS (bulk)   min={diag.min_ess:.0f}   threshold>{ESS_THRESHOLD}  {'OK' if diag.ess_ok else 'FAIL'}\"\n",
    ")\n",
    "print(\n",
    "    f\"Divergences  {diag.divergences}        threshold=0      {'OK' if diag.divergences == 0 else 'FAIL'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full parameter summary — sorted by R-hat descending\n",
    "summary = az.summary(idata)\n",
    "\n",
    "# Structural parameters only (exclude derived mu[date] rows)\n",
    "param_mask = ~summary.index.str.startswith(\"mu[\")\n",
    "param_summary = summary[param_mask].copy()\n",
    "param_summary_sorted = param_summary.sort_values(\"r_hat\", ascending=False)\n",
    "\n",
    "print(f\"Total parameters in model:       {len(summary)}\")\n",
    "print(f\"Structural parameters:           {len(param_summary)}\")\n",
    "print(\n",
    "    f\"With R-hat > {RHAT_THRESHOLD}:           {(param_summary['r_hat'] > RHAT_THRESHOLD).sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"With ESS < {ESS_THRESHOLD}:             {(param_summary['ess_bulk'] < ESS_THRESHOLD).sum()}\"\n",
    ")\n",
    "print()\n",
    "print(\"Top 10 worst parameters by R-hat:\")\n",
    "param_summary_sorted[[\"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\", \"ess_bulk\", \"r_hat\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### R-hat (Gelman-Rubin Statistic)\n",
    "\n",
    "R-hat measures **agreement between chains**. With 4 chains:\n",
    "- R-hat ≈ **1.00** — chains explored the same distribution (converged)\n",
    "- R-hat **1.01–1.1** — mild disagreement, borderline\n",
    "- R-hat **> 1.1** — chains are exploring different regions — the parameter is **not identified** by the data\n",
    "- R-hat **> 2.0** — catastrophic non-convergence (common with unidentifiable models)\n",
    "\n",
    "An R-hat of 4.1 means the between-chain variance is 4× the within-chain variance — the four chains are\n",
    "wandering in completely separate parts of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "rhat_colors = [\n",
    "    \"tomato\" if r > RHAT_THRESHOLD else \"steelblue\"\n",
    "    for r in param_summary_sorted[\"r_hat\"]\n",
    "]\n",
    "param_summary_sorted[\"r_hat\"].plot.bar(\n",
    "    ax=axes[0], color=rhat_colors, edgecolor=\"black\", linewidth=0.4\n",
    ")\n",
    "axes[0].axhline(\n",
    "    RHAT_THRESHOLD,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Threshold ({RHAT_THRESHOLD})\",\n",
    ")\n",
    "axes[0].set_title(\"R-hat per Structural Parameter\")\n",
    "axes[0].set_ylabel(\"R-hat\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=90, labelsize=7)\n",
    "axes[0].legend()\n",
    "\n",
    "ess_colors = [\n",
    "    \"tomato\" if e < ESS_THRESHOLD else \"steelblue\"\n",
    "    for e in param_summary_sorted[\"ess_bulk\"]\n",
    "]\n",
    "param_summary_sorted[\"ess_bulk\"].plot.bar(\n",
    "    ax=axes[1], color=ess_colors, edgecolor=\"black\", linewidth=0.4\n",
    ")\n",
    "axes[1].axhline(\n",
    "    ESS_THRESHOLD,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Threshold ({ESS_THRESHOLD})\",\n",
    ")\n",
    "axes[1].set_title(\"ESS (bulk) per Structural Parameter\")\n",
    "axes[1].set_ylabel(\"ESS\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=90, labelsize=7)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Convergence Metrics by Parameter (red = failed threshold)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Effective Sample Size (ESS)\n",
    "\n",
    "MCMC produces **autocorrelated** samples. ESS is the equivalent number of **independent** samples.\n",
    "With 4 chains × 1000 draws = 4000 total, ideal ESS ≈ 4000. Our ESS of ~4 means the sampler is\n",
    "almost completely stuck — consecutive samples are nearly identical (autocorrelation ≈ 1.0).\n",
    "\n",
    "- **ESS > 400** per parameter: reliable posterior estimates\n",
    "- **ESS < 100**: estimates are noisy and quantiles are unreliable\n",
    "- **ESS < 10**: the sampler is stuck — results are essentially meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ESS (bulk) statistics for structural parameters:\")\n",
    "print(f\"  Min:  {param_summary['ess_bulk'].min():.0f}\")\n",
    "print(f\"  Mean: {param_summary['ess_bulk'].mean():.0f}\")\n",
    "print(f\"  Max:  {param_summary['ess_bulk'].max():.0f}\")\n",
    "print()\n",
    "print(\"Parameters with lowest ESS:\")\n",
    "param_summary.sort_values(\"ess_bulk\")[\n",
    "    [\"mean\", \"sd\", \"ess_bulk\", \"ess_tail\", \"r_hat\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Divergences and Maximum Tree Depth\n",
    "\n",
    "**Divergences** occur when the NUTS sampler's numerical trajectory goes off the rails —\n",
    "the Hamiltonian energy diverges, indicating the sampler hit a curved or narrow region of the posterior.\n",
    "\n",
    "**Max tree depth**: NUTS builds a binary tree of proposal steps. If the tree hits the max depth limit,\n",
    "NUTS is forced to stop, resulting in inefficient exploration. The default is 10 (1024 leapfrog steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(idata, \"sample_stats\"):\n",
    "    ss = idata.sample_stats\n",
    "\n",
    "    # Divergences\n",
    "    if \"diverging\" in ss:\n",
    "        total_div = int(ss[\"diverging\"].sum().values)\n",
    "        div_per_chain = ss[\"diverging\"].sum(dim=\"draw\").values\n",
    "        print(f\"Divergent transitions: {total_div} total\")\n",
    "        print(f\"  Per chain: {div_per_chain}\")\n",
    "    else:\n",
    "        print(\"No divergence info in sample_stats\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Tree depth\n",
    "    if \"tree_depth\" in ss:\n",
    "        max_depth_reached = int(ss[\"tree_depth\"].max().values)\n",
    "        mean_depth = float(ss[\"tree_depth\"].mean().values)\n",
    "        # Default max_treedepth is 10\n",
    "        default_max = 10\n",
    "        pct_at_max = float((ss[\"tree_depth\"] >= default_max).mean().values) * 100\n",
    "        print(\"Tree depth statistics:\")\n",
    "        print(\n",
    "            f\"  Max depth reached: {max_depth_reached} (default limit: {default_max})\"\n",
    "        )\n",
    "        print(f\"  Mean depth: {mean_depth:.1f}\")\n",
    "        print(f\"  % draws at max depth: {pct_at_max:.1f}%\")\n",
    "        if pct_at_max > 5:\n",
    "            print(\n",
    "                \"  WARNING: >5% draws hit max_treedepth — NUTS cannot explore efficiently\"\n",
    "            )\n",
    "            print(\n",
    "                \"  Remedy: increase max_treedepth or reduce step size via higher target_accept\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Visual Diagnostics\n",
    "\n",
    "Numbers tell us *that* convergence failed. Plots tell us *how* it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots: left = marginal KDE per chain, right = samples over time\n",
    "# Converged: KDEs overlap, traces look like 'fuzzy caterpillars'\n",
    "# Non-converged: KDEs are separated, traces drift or get stuck\n",
    "key_params = [\"intercept\", \"adstock_alpha\", \"saturation_lam\", \"saturation_beta\"]\n",
    "\n",
    "axes = az.plot_trace(idata, var_names=key_params, compact=True, figsize=(12, 10))\n",
    "plt.suptitle(\n",
    "    \"Trace Plots — Key Parameters\\n\"\n",
    "    \"(LEFT: KDE per chain should overlap | RIGHT: trace should look like fuzzy caterpillar)\",\n",
    "    y=1.02,\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank plots: uniformly distributed ranks = converged chains\n",
    "# Non-uniform bars = one chain dominates a region of the posterior\n",
    "axes = az.plot_rank(idata, var_names=key_params, kind=\"bars\", figsize=(12, 8))\n",
    "plt.suptitle(\n",
    "    \"Rank Plots — Key Parameters\\n\"\n",
    "    \"(Uniform bars = converged | Peaked/sloped bars = chain imbalance)\",\n",
    "    y=1.02,\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy plot: BFMI (Bayesian Fraction of Missing Information)\n",
    "# Marginal energy distribution should match the transition energy distribution\n",
    "# A mismatch suggests the sampler cannot explore the full energy landscape\n",
    "ax = az.plot_energy(idata, figsize=(10, 4))\n",
    "plt.title(\n",
    "    \"Energy Plot (BFMI Diagnostic)\\n\"\n",
    "    \"Marginal energy (blue) should match transition energy (orange).\\n\"\n",
    "    \"BFMI < 0.3 indicates the sampler cannot freely explore the posterior.\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "try:\n",
    "    bfmi = az.bfmi(idata)\n",
    "    print(f\"BFMI per chain: {bfmi.round(3)}\")\n",
    "    print(\"(Rule of thumb: BFMI < 0.3 is problematic)\")\n",
    "except Exception as e:\n",
    "    print(f\"BFMI calculation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Root Cause Analysis\n",
    "\n",
    "The diagnostic numbers are clear: the model has not converged. But **why?**\n",
    "\n",
    "Let's investigate the data and model structure to understand the fundamental problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT CAUSE 1: Monthly patterns repeat — effective N is only 12\n",
    "# Media spend is distributed pro-rata across weeks within each month,\n",
    "# so all weeks in the same month have IDENTICAL channel spend values.\n",
    "\n",
    "weekly_media = df[[\"Date\"] + config.channel_columns].copy()\n",
    "weekly_media[\"month\"] = pd.to_datetime(weekly_media[\"Date\"]).dt.strftime(\"%Y-%m\")\n",
    "\n",
    "print(\"First 12 rows of weekly media spend:\")\n",
    "print(\"Note how spend values repeat identically within each month.\")\n",
    "print()\n",
    "print(weekly_media.head(12).to_string(index=False))\n",
    "\n",
    "print()\n",
    "unique_patterns = weekly_media[config.channel_columns].drop_duplicates()\n",
    "print(f\"Total rows:              {len(weekly_media)}\")\n",
    "print(f\"Unique media patterns:   {len(unique_patterns)}\")\n",
    "print(f\"Effective N (for media): {len(unique_patterns)}\")\n",
    "print()\n",
    "print(\"The model sees 52 observations but only 12 DISTINCT media inputs.\")\n",
    "print(\"This is the primary driver of non-convergence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT CAUSE 2: Too many parameters for too little data\n",
    "n_channels = len(config.channel_columns)\n",
    "n_params_per_channel = 3  # adstock_alpha, saturation_lam, saturation_beta\n",
    "channel_params = n_channels * n_params_per_channel\n",
    "control_params = len(config.control_columns)\n",
    "other_params = 2  # intercept, sigma\n",
    "total_params = channel_params + control_params + other_params\n",
    "\n",
    "n_obs = len(df)\n",
    "n_effective = 12\n",
    "\n",
    "print(\"Parameter count vs effective observations\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Channels:              {n_channels}\")\n",
    "print(\n",
    "    f\"Params per channel:    {n_params_per_channel} (adstock_alpha, saturation_lam, saturation_beta)\"\n",
    ")\n",
    "print(f\"Channel params:        {channel_params}\")\n",
    "print(f\"Control params:        {control_params}\")\n",
    "print(f\"Intercept + sigma:     {other_params}\")\n",
    "print(f\"Total structural:      {total_params}\")\n",
    "print()\n",
    "print(f\"Total observations:    {n_obs} (weekly rows)\")\n",
    "print(f\"Effective media obs:   {n_effective} (unique monthly patterns)\")\n",
    "print(\n",
    "    f\"Params / effective N:  {total_params} / {n_effective} = {total_params/n_effective:.1f}\"\n",
    ")\n",
    "print()\n",
    "print(\"At 1.4 parameters per effective observation, priors carry most of the\")\n",
    "print(\"information — the likelihood cannot identify all parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT CAUSE 3: Extreme variance in GMV creates a difficult likelihood landscape\n",
    "print(\"GMV distribution characteristics:\")\n",
    "print(f\"  Min:  {y.min():>15,.0f}\")\n",
    "print(f\"  Max:  {y.max():>15,.0f}\")\n",
    "print(f\"  Mean: {y.mean():>15,.0f}\")\n",
    "print(f\"  Std:  {y.std():>15,.0f}\")\n",
    "print(f\"  CV:   {y.std()/y.mean():.2f} (coefficient of variation)\")\n",
    "print(f\"  Max/Min ratio: {y.max()/y.min():.0f}x\")\n",
    "print()\n",
    "print(\"A 25,000x range from min to max creates a highly irregular likelihood.\")\n",
    "print(\"The two extreme outlier months (Oct 2015, Mar 2016) dominate the fit.\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(df[\"Date\"], y, marker=\"o\", linewidth=2)\n",
    "axes[0].set_title(\"Weekly GMV Over Time\")\n",
    "axes[0].set_ylabel(\"GMV\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "axes[1].hist(np.log10(y + 1), bins=15, edgecolor=\"black\")\n",
    "axes[1].set_title(\"log10(GMV) Distribution\")\n",
    "axes[1].set_xlabel(\"log10(GMV + 1)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Remediation Strategies\n",
    "\n",
    "Given the root causes, here are the available levers:\n",
    "\n",
    "| Strategy | What it does | Expected impact |\n",
    "|----------|-------------|----------------|\n",
    "| **Increase `target_accept`** (0.95 → 0.99) | Smaller NUTS steps, fewer tree-depth violations | Reduces max-tree-depth hits |\n",
    "| **Increase `tune`** (2000 → 3000) | More adaptation iterations to find good step size | Better mass matrix estimate |\n",
    "| **Tighter `saturation_beta` prior** (HalfNormal(2) → HalfNormal(0.5)) | Constrains channel contributions to plausible range | Reduces posterior volume |\n",
    "| **Reduce `adstock_max_lag`** (4 → 2) | Fewer adstock parameters | Not tried — already at 4 |\n",
    "| **Use monthly data** | 12 rows, 1:1 with media | Lose weekly variation |\n",
    "\n",
    "### Why tighten `saturation_beta`?\n",
    "\n",
    "PyMC-Marketing scales all inputs to [0, 1] via MaxAbsScaler. After scaling:\n",
    "- The target `total_gmv` is in [0, 1]\n",
    "- Channel contributions must sum to roughly 1.0 (plus intercept)\n",
    "- `HalfNormal(sigma=2)` puts most prior mass above 1.0 — way too wide\n",
    "- `HalfNormal(sigma=0.5)` concentrates mass in [0, 0.5] per channel — much more realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prior comparison: HalfNormal(2) vs HalfNormal(0.5)\n",
    "from scipy.stats import halfnorm\n",
    "\n",
    "x_prior = np.linspace(0, 5, 300)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(\n",
    "    x_prior,\n",
    "    halfnorm.pdf(x_prior, scale=2),\n",
    "    label=\"HalfNormal(sigma=2) — v1\",\n",
    "    color=\"tomato\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.plot(\n",
    "    x_prior,\n",
    "    halfnorm.pdf(x_prior, scale=0.5),\n",
    "    label=\"HalfNormal(sigma=0.5) — v2\",\n",
    "    color=\"steelblue\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.axvline(\n",
    "    1.0, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Max scaled contribution = 1.0\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Prior for saturation_beta: v1 vs v2\")\n",
    "ax.set_xlabel(\"saturation_beta value\")\n",
    "ax.set_ylabel(\"Prior density\")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"P(beta > 1.0) under HalfNormal(2):   {halfnorm.sf(1.0, scale=2):.1%}\")\n",
    "print(f\"P(beta > 1.0) under HalfNormal(0.5): {halfnorm.sf(1.0, scale=0.5):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build V2 model with improved settings\n",
    "config_v2 = ModelConfig(\n",
    "    target_accept=0.99,  # was 0.95\n",
    "    tune=3000,  # was 2000\n",
    "    draws=1000,\n",
    ")\n",
    "\n",
    "# Override saturation_beta prior only\n",
    "model_config_v2 = config_v2.get_model_config()\n",
    "model_config_v2[\"saturation_beta\"] = Prior(\"HalfNormal\", sigma=0.5)\n",
    "\n",
    "mmm_v2 = MMM(\n",
    "    date_column=config_v2.date_column,\n",
    "    channel_columns=config_v2.channel_columns,\n",
    "    control_columns=config_v2.control_columns,\n",
    "    adstock=GeometricAdstock(l_max=config_v2.adstock_max_lag),\n",
    "    saturation=LogisticSaturation(),\n",
    "    model_config=model_config_v2,\n",
    ")\n",
    "\n",
    "print(\"V2 config:\")\n",
    "print(f\"  target_accept:    {config_v2.target_accept}  (was 0.95)\")\n",
    "print(f\"  tune:             {config_v2.tune}   (was 2000)\")\n",
    "print(\"  saturation_beta:  HalfNormal(sigma=0.5)  (was HalfNormal(sigma=2))\")\n",
    "print()\n",
    "print(\"V2 model initialized. Running MCMC sampling (approx 4-6 minutes)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "idata_v2 = fit_model(mmm_v2, X, y, config_v2)\n",
    "print(\"V2 sampling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 diagnostics\n",
    "diag_v2 = check_convergence(idata_v2)\n",
    "\n",
    "print(\"V2 DIAGNOSTICS:\")\n",
    "print(f\"  Passed:     {'PASSED' if diag_v2.passed else 'FAILED'}\")\n",
    "print(f\"  Max R-hat:  {diag_v2.max_rhat:.4f}\")\n",
    "print(f\"  Min ESS:    {diag_v2.min_ess:.0f}\")\n",
    "print(f\"  Divergences:{diag_v2.divergences}\")\n",
    "\n",
    "print()\n",
    "axes = az.plot_trace(idata_v2, var_names=key_params, compact=True, figsize=(12, 10))\n",
    "plt.suptitle(\"V2 Trace Plots\", y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before / after comparison\n",
    "diag_v1 = check_convergence(idata)\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"V1 (original)\": {\n",
    "            \"max R-hat\": f\"{diag_v1.max_rhat:.4f}\",\n",
    "            \"min ESS\": f\"{diag_v1.min_ess:.0f}\",\n",
    "            \"divergences\": diag_v1.divergences,\n",
    "            \"passed\": diag_v1.passed,\n",
    "            \"target_accept\": 0.95,\n",
    "            \"tune\": 2000,\n",
    "            \"saturation_beta prior\": \"HalfNormal(sigma=2)\",\n",
    "        },\n",
    "        \"V2 (improved)\": {\n",
    "            \"max R-hat\": f\"{diag_v2.max_rhat:.4f}\",\n",
    "            \"min ESS\": f\"{diag_v2.min_ess:.0f}\",\n",
    "            \"divergences\": diag_v2.divergences,\n",
    "            \"passed\": diag_v2.passed,\n",
    "            \"target_accept\": config_v2.target_accept,\n",
    "            \"tune\": config_v2.tune,\n",
    "            \"saturation_beta prior\": \"HalfNormal(sigma=0.5)\",\n",
    "        },\n",
    "    }\n",
    ").T\n",
    "\n",
    "print(\"BEFORE vs AFTER COMPARISON:\")\n",
    "print(comparison.to_string())\n",
    "print()\n",
    "\n",
    "if diag_v2.passed:\n",
    "    print(\"V2 model converged! Results in notebooks 04-05 are reliable.\")\n",
    "else:\n",
    "    print(\n",
    "        \"V2 still shows convergence issues — this reflects fundamental data limitations.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"R-hat {diag_v2.max_rhat:.3f} vs {diag_v1.max_rhat:.3f} (v1) — some improvement.\"\n",
    "    )\n",
    "    print(\"Posteriors are influenced primarily by priors, not data.\")\n",
    "    print(\"Interpretations in notebooks 04-05 must be treated with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_path_v2 = OUTPUTS_DIR / \"models\" / f\"mmm_fit_{date_str}_v2.nc\"\n",
    "model_path_v2.parent.mkdir(parents=True, exist_ok=True)\n",
    "mmm_v2.save(str(model_path_v2))\n",
    "print(f\"V2 model saved to: {model_path_v2}\")\n",
    "print()\n",
    "print(\"Notebooks 04 and 05 will load the most recent .nc file (this v2 model).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Conclusions\n",
    "\n",
    "### What we found\n",
    "\n",
    "The V1 model failed convergence severely (R-hat up to 4.26, ESS as low as 4). The V2 model\n",
    "shows improvement. Whether it fully converges depends on the specific data and random seed.\n",
    "\n",
    "### Root causes (ranked by severity)\n",
    "\n",
    "1. **Effective N = 12** — media spend only has 12 unique monthly patterns driving 52 weekly rows.\n",
    "   The likelihood cannot identify 17 structural parameters from 12 data points.\n",
    "2. **GMV variance is extreme** — the 25,000x range between min and max weeks creates a\n",
    "   difficult likelihood landscape that NUTS struggles to explore.\n",
    "3. **Prior too wide** — `saturation_beta ~ HalfNormal(2)` allows contributions far outside the\n",
    "   plausible range for MaxAbsScaled data, inflating the posterior volume.\n",
    "\n",
    "### What the v2 changes did\n",
    "\n",
    "- **Tighter prior** reduces the volume the sampler needs to explore\n",
    "- **Higher `target_accept`** forces smaller, more careful NUTS steps\n",
    "- **More tuning** gives the mass matrix estimator more data to adapt to the posterior shape\n",
    "\n",
    "### What we cannot fix\n",
    "\n",
    "With only 12 effective observations for 17 parameters, the posterior will always be\n",
    "**prior-dominated** to some degree. This is not a modeling failure — it is an honest\n",
    "reflection of the data. Bayesian priors encode domain knowledge precisely for this situation.\n",
    "\n",
    "### Recommendation for production\n",
    "\n",
    "- Use **daily data with daily media spend** (if available) to get effective N > 100\n",
    "- Use **hierarchical priors** across channels to share information\n",
    "- Use **informative priors** grounded in industry benchmarks for adstock and saturation\n",
    "- Consider **log-transforming the target** to reduce the extreme variance\n",
    "\n",
    "> **For this demo:** we proceed with the V2 model in notebooks 04 and 05,\n",
    "> clearly caveating that results are prior-dominated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
